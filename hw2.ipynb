{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anak Agung Ngurah Bagus Trihatmaja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6.1\n",
    "\n",
    "> Write a code in Python whose input is a training dataset {(x1, y1), . . . , (xN , yN )} and its output is the weight vector θ in the linear regression model y = θ'φ(x), for a given nonlinear mapping φ(·). Implement two cases: i) using the closed-form solution, ii) using a stochastic gradient descent on mini-batches of size m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import scipy.io\n",
    "import math\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function generates data based on the formula we input \n",
    "# For this time we will generate a square function added with a noise\n",
    "f = lambda  x: x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closed function regression\n",
    "def closed_function_regression(train_X, train_Y):\n",
    "  X = np.array(train_X)\n",
    "  y = np.array(train_Y)\n",
    "  \n",
    "  \n",
    "  # TODO: Check again here  \n",
    "  # ones = np.ones(len(X))\n",
    "  # X = np.column_stack((ones, X))\n",
    "  y = np.array(y)\n",
    "        \n",
    "  Xt = transpose(X)\n",
    "  product = dot(Xt, X)\n",
    "  theInverse = inv(product)\n",
    "  w = dot(dot(theInverse, Xt), y)\n",
    "        \n",
    "  return w       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(train_X, train_Y, batch_size):\n",
    "    X = train_X\n",
    "    y = train_Y\n",
    "    \n",
    "    random_indices = np.random.choice(len(X), len(y), replace=False)\n",
    "    \n",
    "    X_shuffled = X[random_indices,:]\n",
    "    y_shuffled = y[random_indices]\n",
    "    mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                   i in range(0, len(y), batch_size)]\n",
    "    \n",
    "    return mini_batches\n",
    "    \n",
    "\n",
    "def mini_gradient_descent(train_X, train_Y, learning_rate, num_iter, batch_size):\n",
    "    # Prepare the data\n",
    "    X = np.array(train_X)\n",
    "    y = np.array(train_Y)\n",
    "    \n",
    "    # TODO: Check why do we need to put 1 here\n",
    "    # ones = np.ones(len(X))\n",
    "    # X = np.column_stack((ones, X))\n",
    "    # y = np.array(y)\n",
    "    \n",
    "    # Randomize the data and split into samples\n",
    "    df = shuffle_data(X, y, batch_size)\n",
    "    \n",
    "    # Get the dimension\n",
    "    x_col = X.shape[1]\n",
    "    \n",
    "    # Set the initial theta and other initial variables\n",
    "    theta = np.zeros((x_col, 1))\n",
    "    \n",
    "    start_i = 0\n",
    "    \n",
    "    # Create the loop\n",
    "    for i in range(start_i + 1, num_iter + 1):\n",
    "        # For each randomized-mini-batch partition\n",
    "        for j in range(0, len(df)):\n",
    "            X = df[j][0]\n",
    "            y = df[j][1]\n",
    "            \n",
    "            y_hat = np.dot(X, theta)\n",
    "            \n",
    "            # Break if overflow occurs\n",
    "            theta = theta - learning_rate * np.dot(X.T, y_hat - y)\n",
    "            \n",
    "            # FIXME: Break if NaN occurs\n",
    "            # if not (float('-inf') < float(temp[0]) < float('inf')): \n",
    "            #    return theta\n",
    "                \n",
    "            # theta =  temp\n",
    "            \n",
    "        \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6.2\n",
    "\n",
    "> Consider n-degree polynomials, φ(·) =  1 x x^2 · · · x^n . Download the dataset on the course webpage and work with ‘dataset1’. Run the code on the training data to compute θ for n ∈ {2, 3, 5}. Evaluate the regression error on both training and the test data. Report θ, training error and test error for both implementation (closed-form vs gradient descent). What is the effect of the size of the mini-batch on the speed and testing error of the solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.40148277]])"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = scipy.io.loadmat(\"/Users/bagustrihatmaja/Downloads/HW1_Data/dataset1.mat\")\n",
    "\n",
    "\n",
    "X_trn = df['X_trn']\n",
    "Y_trn = df['Y_trn']\n",
    "X_test = df['X_tst']\n",
    "Y_test = df['Y_tst']\n",
    "\n",
    "\n",
    "closed_function_regression(X_trn, Y_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_polinomial_array(element, n):\n",
    "    pol = np.empty([1, n+1])\n",
    "    # Debug\n",
    "    # print(pol)\n",
    "    for i in range(0, n+1):\n",
    "        # Debug\n",
    "        # print(\"Element: \\n\", element)\n",
    "        pol[0][i] = element ** i\n",
    "        \n",
    "    return pol[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the degree of polynomial        \n",
    "degree = 5\n",
    "# degree + 1 for 1's in the first column\n",
    "X_trn_new = np.ndarray((len(X_trn), degree + 1))\n",
    "\n",
    "# print(X_trn_new)\n",
    "\n",
    "i = 0\n",
    "X = np.array(X_trn)\n",
    "for x in X:\n",
    "    X_trn_new[i] = create_polinomial_array(x, degree)\n",
    "    i += 1\n",
    "        \n",
    "\n",
    "mini_gradient_descent(X_trn, Y_trn, 0.001, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysing the regression error and speed\n",
    "# Hyper-parameter are the alpha (learning rate), the batch size and the number of iterations\n",
    "def analyze(learning_rate, n_iteration, batch_size):\n",
    "    degrees = [2, 3, 5]\n",
    "    \n",
    "    # Time for performance\n",
    "    grad_desc_performance = []\n",
    "    \n",
    "    # For train data\n",
    "    error_closed_form_train = []\n",
    "    error_grad_desc_train = []\n",
    "    \n",
    "    # For test data\n",
    "    error_closed_form_test = []\n",
    "    error_grad_desc_test = []\n",
    "    \n",
    "    X = np.array(X_trn)\n",
    "    X_tst = np.array(X_test)\n",
    "    \n",
    "    for degree in degrees:\n",
    "        X_trn_new = np.ndarray((len(X_trn), degree + 1))\n",
    "        X_tst_new = np.ndarray((len(X_tst), degree + 1))\n",
    "        \n",
    "        i = 0\n",
    "        for x in X:\n",
    "            X_trn_new[i] = create_polinomial_array(x, degree)\n",
    "            i += 1\n",
    "        \n",
    "        i = 0\n",
    "        for x in X_tst:\n",
    "            X_tst_new[i] = create_polinomial_array(x, degree)\n",
    "            i += 1\n",
    "        \n",
    "        tetha_closed_form = closed_function_regression(X_trn_new, Y_trn)\n",
    "        t = time.process_time()\n",
    "        tetha_grad_desc = mini_gradient_descent(X_trn_new, Y_trn, learning_rate, n_iteration, batch_size)\n",
    "        elapsed_time = time.process_time() - t\n",
    "        print(\"Theta for degree {} done in: {}\\n\".format(degree, elapsed_time))\n",
    "        \n",
    "        \n",
    "        y_hat_closed_form_train = np.dot(X_trn_new, tetha_closed_form)\n",
    "        y_hat_grad_desc_train = np.dot(X_trn_new, tetha_grad_desc)\n",
    "        \n",
    "        y_hat_closed_form_test = np.dot(X_tst_new, tetha_closed_form)\n",
    "        y_hat_grad_desc_test = np.dot(X_tst_new, tetha_grad_desc)\n",
    "        \n",
    "        error_closed_form_train.append(np.sqrt(((Y_trn - y_hat_closed_form_train) ** 2).mean()))\n",
    "        error_grad_desc_train.append(np.sqrt(((Y_trn - y_hat_grad_desc_train) ** 2).mean()))\n",
    "        \n",
    "        error_closed_form_test.append(np.sqrt(((Y_test - y_hat_closed_form_test) ** 2).mean()))\n",
    "        error_grad_desc_test.append(np.sqrt(((Y_test - y_hat_grad_desc_test) ** 2).mean()))\n",
    "        \n",
    "    d = {\n",
    "        'degree': [2, 3, 5],\n",
    "        'close_form_error_train': error_closed_form_train,\n",
    "        'grad_desc_error_train': error_grad_desc_train,\n",
    "        'close_form_error_test': error_closed_form_test,\n",
    "        'grad_desc_error_test': error_grad_desc_test\n",
    "    }\n",
    "    errors = pd.DataFrame(data = d)\n",
    "    return errors\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta for degree 2 done in: 0.007232999999999379\n\nTheta for degree 3 done in: 0.0033169999999955735\n\nTheta for degree 5 done in: 0.00168699999998978\n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_form_error_test</th>\n",
       "      <th>close_form_error_train</th>\n",
       "      <th>degree</th>\n",
       "      <th>grad_desc_error_test</th>\n",
       "      <th>grad_desc_error_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.434831</td>\n",
       "      <td>4.974125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.538616e+01</td>\n",
       "      <td>1.652118e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.336971</td>\n",
       "      <td>1.991950</td>\n",
       "      <td>3</td>\n",
       "      <td>1.865774e+01</td>\n",
       "      <td>1.167766e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.442782</td>\n",
       "      <td>1.986676</td>\n",
       "      <td>5</td>\n",
       "      <td>5.080623e+90</td>\n",
       "      <td>2.000555e+89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_form_error_test</th>\n",
       "      <th>close_form_error_train</th>\n",
       "      <th>degree</th>\n",
       "      <th>grad_desc_error_test</th>\n",
       "      <th>grad_desc_error_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.434831</td>\n",
       "      <td>4.974125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.538616e+01</td>\n",
       "      <td>1.652118e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.336971</td>\n",
       "      <td>1.991950</td>\n",
       "      <td>3</td>\n",
       "      <td>1.865774e+01</td>\n",
       "      <td>1.167766e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.442782</td>\n",
       "      <td>1.986676</td>\n",
       "      <td>5</td>\n",
       "      <td>5.080623e+90</td>\n",
       "      <td>2.000555e+89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size 10\n",
    "result1 = analyze(0.00001, 10, 10)\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta for degree 2 done in: 0.0021930000000054406\n\nTheta for degree 3 done in: 0.0027790000000038617\n\nTheta for degree 5 done in: 0.0026310000000080436\n\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_form_error_test</th>\n",
       "      <th>close_form_error_train</th>\n",
       "      <th>degree</th>\n",
       "      <th>grad_desc_error_test</th>\n",
       "      <th>grad_desc_error_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.434831</td>\n",
       "      <td>4.974125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.064596e+01</td>\n",
       "      <td>1.862262e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.336971</td>\n",
       "      <td>1.991950</td>\n",
       "      <td>3</td>\n",
       "      <td>2.631678e+01</td>\n",
       "      <td>1.405648e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.442782</td>\n",
       "      <td>1.986676</td>\n",
       "      <td>5</td>\n",
       "      <td>6.622336e+46</td>\n",
       "      <td>2.415631e+45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close_form_error_test</th>\n",
       "      <th>close_form_error_train</th>\n",
       "      <th>degree</th>\n",
       "      <th>grad_desc_error_test</th>\n",
       "      <th>grad_desc_error_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66.434831</td>\n",
       "      <td>4.974125</td>\n",
       "      <td>2</td>\n",
       "      <td>7.064596e+01</td>\n",
       "      <td>1.862262e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.336971</td>\n",
       "      <td>1.991950</td>\n",
       "      <td>3</td>\n",
       "      <td>2.631678e+01</td>\n",
       "      <td>1.405648e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.442782</td>\n",
       "      <td>1.986676</td>\n",
       "      <td>5</td>\n",
       "      <td>6.622336e+46</td>\n",
       "      <td>2.415631e+45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size 20\n",
    "result1 = analyze(0.00001, 6, 20)\n",
    "result1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the hyperparameter of `number of iteration` is 10 and `learning rate` is 0.00001 and we split the data by 10, so we get 12 partition. We get smaller RMSE compared to the closed form for our training data.\n",
    "\n",
    "The error we get varies depends on our hyperparameters we mention above. In case of polinomial of 5, we get better result if we increase the batch size and adjust the number of iteration accordingly.\n",
    "\n",
    "With batch size of 10, we get the time decreasing from degree 2, 3 to 5 but for batch size of 20, the time is similar, except for degree 2. Therefore, we can conclude there is no significant influence of the batch time to the performance in this case.\n",
    "\n",
    "We have known bug that if we set the learning rate or the number of iteration too high, we will overflow exception (known bugs are marked as `# FIXME` in this report)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6.3\n",
    "\n",
    "> Download the dataset on the course webpage and work with ‘dataset2’. Write a code in Python that applies Ridge regression to the dataset to compute θ for a given λ. Implement two cases:\n",
    " 2\n",
    "using a closed-form solution and using a stochastic gradient descent method with mini-batches of size m. Use K-fold cross validation on the training dataset to obtain the best regularization λ and apply the optimal θ to compute the regression error on test samples. Report the optimal λ, θ, test and training set errors for K ∈ {2,10,N}, where N is the number of samples. In all cases try n ∈ {2, 3, 5}. How does the test error change as a function of λ and n?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RidgeRegression(object):\n",
    "    def __init__(self, lmbda=0.1):\n",
    "        self.lmbda = lmbda\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        C = X.T.dot(X) + self.lmbda * np.eye(X.shape[1])\n",
    "        self.w = np.linalg.inv(C).dot(X.T.dot(y))\n",
    "    \n",
    "    def set_params(self, lmbda=0.1):\n",
    "        self.lmbda = lmbda\n",
    "        return self\n",
    "        \n",
    "    def get_weight(self):\n",
    "        return self.w\n",
    "    \n",
    "    # def get_error(self):\n",
    "    #     y_hat = np.dot(self.X, self.w)\n",
    "    #     return np.sqrt(((self.Y - y_hat) ** 2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimisation(object):\n",
    "    def __init__(self, X, y, model):\n",
    "        self.model = model\n",
    "        # Prepare the data\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        \n",
    "    def closed_form(self):\n",
    "        self.model.fit(self.X, self.y)\n",
    "        return self.model.get_weight()\n",
    "    \n",
    "    def mini_batch_stochastic(self, batch_size):\n",
    "        y = np.array(y)\n",
    "    \n",
    "        # Randomize the data and split into samples\n",
    "        df = shuffle_data(X, y, batch_size)\n",
    "    \n",
    "        # Get the dimension\n",
    "        x_dim = X.shape[1]\n",
    "    \n",
    "        # Set the initial tetha and other initial variables\n",
    "        theta = np.zeros((x_dim, 1))\n",
    "    \n",
    "        start_i = 0\n",
    "    \n",
    "        # Create the loop\n",
    "        for i in range(start_i + 1, num_iter + 1):\n",
    "            # For each randomized-mini-batch partition\n",
    "            for j in range(0, len(df)):\n",
    "                X = df[j][0]\n",
    "                y = df[j][1]\n",
    "                \n",
    "                self.model.fit(self.X, self.y)\n",
    "                theta = theta - learning_rate * self.model.get_weight \n",
    "        \n",
    "        return theta\n",
    "        \n",
    "    def shuffle_data(self, batch_size):\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "    \n",
    "        random_indices = np.random.choice(len(X), len(y), replace=False)\n",
    "    \n",
    "        X_shuffled = X[random_indices,:]\n",
    "        y_shuffled = y[random_indices]\n",
    "        mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                   i in range(0, len(y), batch_size)]\n",
    "        return mini_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = scipy.io.loadmat(\"/Users/bagustrihatmaja/Downloads/HW1_Data/dataset2.mat\")\n",
    "\n",
    "\n",
    "X_trn2 = df2['X_trn']\n",
    "Y_trn2 = df2['Y_trn']\n",
    "\n",
    "\n",
    "\n",
    "X_test2 = df2['X_tst']\n",
    "Y_test2 = df2['Y_tst']\n",
    "\n",
    "\n",
    "ridge = RidgeRegression()\n",
    "m = Minimisation(X_trn2, Y_trn2, ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(X, y, lmbda, fold):\n",
    "    # Split the data into fold part\n",
    "    # Prepare the data\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    ridge = RidgeRegression()\n",
    "    \n",
    "    # Randomize the data and split into samples\n",
    "    batch_size = int(len(X) / fold)\n",
    "    df = shuffle_data(X, y, batch_size)\n",
    "    \n",
    "    test_result = []\n",
    "    \n",
    "    for i in range(0, len(df)):\n",
    "        # Temporary data for k-fold\n",
    "        temp = df.copy()\n",
    "        \n",
    "        # To store partitions for training\n",
    "        train = []\n",
    "        \n",
    "        # Save and pop an element for test\n",
    "        test = temp.pop(i)\n",
    "        \n",
    "        # Concat the remaining array\n",
    "        X_train = np.empty(shape=[X.shape[0], X.shape[1]])\n",
    "        Y_train = np.empty(shape=[y.shape[0], y.shape[1]])\n",
    "        \n",
    "        for j in range(0, len(df)):\n",
    "            _X = df[j][0]\n",
    "            _y = df[j][1]\n",
    "            X_train = np.concatenate((X_train, _X))\n",
    "            Y_train = np.concatenate((Y_train, _y))\n",
    "        \n",
    "        \n",
    "        ridge.set_params(lmbda)\n",
    "        # test[0] is X in test data\n",
    "        # test[1] is Y in test data\n",
    "        \n",
    "                            \n",
    "        ridge.fit(X_train, Y_train)\n",
    "        #print(X_train, Y_train)\n",
    "        w = ridge.get_weight()\n",
    "        \n",
    "        if ~(math.isnan(w[0])):\n",
    "            y_hat = np.dot(test[0], w)\n",
    "            test_result.append(np.sqrt(((test[1] - y_hat) ** 2).mean()))\n",
    "    \n",
    "    return np.nanmean(test_result)\n",
    "\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_analysis(degree, fold):\n",
    "    X = np.array(X_trn2)\n",
    "    k_fold_result = []\n",
    "    \n",
    "    X_trn_new = np.ndarray((len(X_trn2), degree + 1))\n",
    "    i = 0\n",
    "    for x in X:\n",
    "        X_trn_new[i] = create_polinomial_array(x, degree)\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    for j in range(-10, 2):\n",
    "        k_fold_result.append(k_fold(X_trn_new, Y_trn2, 10 ** j, fold))\n",
    "    \n",
    "    return k_fold_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial degree: 2, K-Fold: 2\n\nMinimum error at index: 2 value 63.90616407539763 with lambda: 10^-8\n\nPolynomial degree: 2, K-Fold: 10\n\nMinimum error at index: 1 value 40.74383837886053 with lambda: 10^-9\n\nPolynomial degree: 2, K-Fold: 20\n\nMinimum error at index: 9 value 58.46242254776773 with lambda: 10^-1\n\nPolynomial degree: 3, K-Fold: 2\n\nMinimum error at index: 7 value 15.758953693836744 with lambda: 10^-3\n\nPolynomial degree: 3, K-Fold: 10\n\nMinimum error at index: 6 value 30.238897004998698 with lambda: 10^-4\n\nPolynomial degree: 3, K-Fold: 20\n\nMinimum error at index: 3 value 26.10703357540074 with lambda: 10^-7\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial degree: 5, K-Fold: 2\n\nMinimum error at index: 0 value 15.561790112626733 with lambda: 10^-10\n\nPolynomial degree: 5, K-Fold: 10\n\nMinimum error at index: 2 value 29.488860256919907 with lambda: 10^-8\n\nPolynomial degree: 5, K-Fold: 20\n\nMinimum error at index: 6 value 15.644992536061014 with lambda: 10^-4\n\n"
     ]
    }
   ],
   "source": [
    "degrees = [2, 3, 5]\n",
    "folds = [2, 10, 20]\n",
    "\n",
    "for degree in degrees:\n",
    "    for fold in folds:\n",
    "        temp = k_fold_analysis(degree, fold)\n",
    "        print(\"Polynomial degree: {}, K-Fold: {}\\n\".format(degree, fold))\n",
    "        print(\"Minimum error at index: {} value {} with lambda: 10^{}\\n\".format(\n",
    "            np.argmin(temp), np.min(temp), (np.argmin(temp) - 10)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we see that the best lambda is depending upon the number of parameters in X--polynomial with degree 5 has different lambda that result the smallest error compared to other polynomial degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
